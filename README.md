# Проект по Automatic Speech Recognition

Работа выполнена в рамках курса по глубинному обучению в обработке звука: [задание](https://github.com/markovka17/dla/tree/2024/hw1_asr).

Пайплайн обучения ASR-модели с логированием, сохранением чекпоинтов (в т.ч. и в Google Drive) и вычислением метрик.
Далее описана процедура запуска предобученной в этом пайплайне модели на произвольных данных, а также воспроизведения этой модели.

Была использована архитектура [DeepSpeech 2](https://arxiv.org/abs/1512.02595), однако пайплайн может использоваться и для других архитектур.

## Подготовка

Код проверялся на версии Python 3.10.

Предварительно необходимо установить все необходимые пакеты:
```
pip install -r requirements.txt
```

## Запуск обученной модели

### Загрузка весов

Для загрузки весов можно использовать скрипт [model_loader.py](./scripts/model_loader):
```
python3 scripts/model_loader/model_loader.py download best
```
В результате будет загружен лучший чекпоинт, выбранный среди ряда экспериментов:

|                         | CER    | WER    |
|-------------------------|--------|--------|
| LibriSpeech: test-clean | 0.0703 | 0.1605 |
| LibriSpeech: test-other | 0.1904 | 0.3595 |

> Метрики были вычислены при использовании beam search + LM для генерации текста по выходам модели

Также с помощью аргумента `-p, --path` можно указать директорию сохранения чекпоинта (по умолчанию: `saved/models`).

В случае каких-либо проблем со стороны API Google Drive загрузить модель можно вручную по [ссылке](https://drive.google.com/drive/folders/1k7JkQV9ZBwQTKEYfJqt78gI5ko6NtYN-?usp=drive_link).

### Инференс модели

Для инференса можно использовать скрипт [`test.py`](test.py), в котором:

- Аргумент `-r, --resume`: путь до используемого чекпоинта модели.
> При загрузке чекпоинта с помощью `model_loader.py` полный путь до чекпоинта выводится в консоль

- Аргумент `-c, --config`: путь до дополнительного конфига (конфиг инференса).\
\
Основной конфиг возьмётся из той же директории, что и указанный чекпоинт.
Итоговый конфиг берётся как основной, объединённый с дополнительным (в случае одинаковых полей берётся значение из дополнительного конфига).\
\
В конфиге инференса указываются параметры инференса, например, языковая модель, используемая для генерации текста по выходным логитам модели, и размер батча, а также, возможно, данные, на которых осуществляется инференс.
Примеры таких конфигов можно найти в [`hw_asr/configs/eval_metrics_configs`](hw_asr/configs/eval_metrics_configs).

> В конфигах [`test-clean.json`](hw_asr/configs/eval_metrics_configs/test-clean.json) и [`test-other.json`](hw_asr/configs/eval_metrics_configs/test-other.json) указаны параметры для языковой модели, используемой для декодирования, которые были подобраны по датасету Librispeech dev-clean.
Предположительно, с этими же параметрами результат будет лучше и на других данных.

Данные, на которых нужно осуществить инференс (распознавание речи), можно указать:
- в качестве датасета в конфиге инференса, как, например, в [`test-clean.json`](hw_asr/configs/eval_metrics_configs/test-clean.json) (поддерживаемые датасеты можно найти [тут](hw_asr/datasets))
- с помощью аргумента (`-t`, `--test-data-folder`), указав путь до директории, имеющей следующую структуру:
```
test_dir
|-- audio
|    |-- voice1.[wav|mp3|flac|m4a]
|    |-- voice2.[wav|mp3|flac|m4a]
|-- transcriptions
|    |-- voice1.txt
|    |-- voice2.txt
```

Файлы в поддиректории `transcriptions`, как и сама эта директория, являются опциональными и нужны лишь для подсчёта CER и WER.

Пример запуска на датасете Librispeech (test-clean):
```
python3 test.py \
   --config=hw_asr/configs/eval_metrics_configs/test-clean.json \
   --resume=saved/models/$EXP/$RUN/$CHECKPOINT.pth
```

Пример запуска на наборе аудиозаписей:
```
python3 test.py \
   --config=hw_asr/configs/eval_metrics_configs/test-clean.json \
   --resume=saved/models/$EXP/$RUN/$CHECKPOINT.pth \
   --test-data-folder=test_data
```

(дополнительный конфиг тут передаётся, чтобы задать используемый декодер логитов в текст)

> Языковая модель, используемая для декодирования, и данные (при инференсе на одном из датасетов) будут загружены автоматически при запуске скрипта.

В результате, если были даны целевые результаты транскрибации, будут напечатаны значения метрик CER и WER, усреднённые по всем аудиозаписям, при разных вариантах декодирования предсказанного текста по вероятностям, выданным моделью (argmax, beam search, beam search + LM).
Также в файл, переданный как аргумент `-o, --output` (по умолчанию - `"output.json"`) будут записаны предсказания модели вместе с верными ответами из датасета.

> Для сгенерированных с помощью beam search результатов транскрибации в файл также будет записана их вер-ть согласно выходным логитам модели.

## Как воспроизвести обучение

### Общие сведения

Весь процесс обучения определяется конфигом в формате JSON ([примеры](hw_asr/configs/)).
Запустить обучение можно вызовом скрипта [train.py](./train.py), указав путь до конфига в качестве аргумента `-c, --config`.

Для дообучения модели можно указать путь до неё в качестве аргумента `-r, --resume`.
Тогда изначальные веса модели будут загружены из чекпоинта вместо того, чтобы быть случайно инициализированными.

### Воспроизведение лучшего чекпоинта

Обучение осуществлялось вызовом команды

```
python3 train.py \
   --config=hw_asr/configs/1+6/kaggle_deepspeech2_1+6_bidir_gru.json
```

И затем (для дообучения на Librispeech test-other) вызовом
```
python3 test.py \
   --config=hw_asr/configs/1+6/kaggle_deepspeech2_1+6_other_finetuning.json \
   --resume=saved/models/kaggle_deepspeech2_1+6/$RUN_NAME/model_best.pth
```
где $RUN_NAME - имя запуска в первом случае

> Все необходимые для обучения данные будут загружены автоматически.

В конфигах указаны некоторые пути до датасетов (`data_dir`, `index_dir`), которые использовались в авторском окружении (при обучении на Kaggle). Их можно полностью удалить из конфига или заменить на свои.

В конфигах указано сохранение чекпоинтов на Google Drive (секция "external storage"). Чтобы использовать его для своего личного аккаунта, следуйте инструкциям [отсюда](docs/gdrive_storage.md#access-to-a-personal-google-drive) и внесите в секцию "external storage" конфига соответствующие изменения (указав id папки на Google Drive и путь до файла с credentials). Также можно отключить экспорт чекпоинтов на Google Drive, удалив из конфига обучения секцию "external storage".

Для логирования используется [WandB](https://wandb.ai/), для чего необходимо либо войти в свой аккаунт, используя `wandb login`, либо указать свой токен (authentication key) в переменной окружения [`WANDB_API_KEY`](https://docs.wandb.ai/guides/track/environment-variables/).

## Тестирование реализации

В [`hw_asr/tests`](hw_asr/tests) представлены тесты для проверки работоспособности кода. Можно запустить их все командой:
```
python3 -m unittest discover hw_asr/tests
```

## Структура репозитория
- `hw_asr`
1. `hw_asr/configs`: файлы конфигурации

файл конфигурации (JSON) полностью определяет процесс обучения (какая модель берётся, какие данные используются, как и какие логируются метрики)

2. `hw_asr/tests`: тесты реализации

Можно запустить все командой
```
python3 -m unittest discover hw_asr/tests
```

3. остальные поддиректории: исходный код пайплайна

- `notebooks`: Jupyter-ноутбуки с демонстрацией какой-либо функциональности пайплайна (сейчас там только аугментации)

#### Скрипты

Подробнее про каждый из них можно узнать, вызвав `python3 {script_name} --help`.

- `train.py`: запуск процедуры обучения из консоли

- `test.py`: запуск оценки сохранённой модели из консоли

- `scripts/model_loader/model_loader.py`: импорт моделей из внешнего хранилища (поддержка реализована только для Google Drive)

#### Файлы для импорта моделей из Google Drive:

Используются в скрипте `model_loader.py` для загрузки чекпоинтов из авторского хранилища.

- `gdrive_storage/external_storage.json`: файл конфигурации внешнего хранилища, в котором указывается директория на Google Drive и путь до файла с credentials (подробнее - [тут](./docs/gdrive_storage.md))

## Автор

Егоров Егор:
- tg: [@TrickmanOff](https://t.me/TrickmanOff)
- e-mail: yegyegorov@gmail.com